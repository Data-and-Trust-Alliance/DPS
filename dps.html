<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Data Provenance Standard</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Data Provenance Standard</h1>
</header>
<p><strong>The Data Provenance Standard</strong></p>
<p>The Data Provenance Standard defines metadata about a data set, so
that the data set provenance and lineage are understood. This set of
metadata are necessary to enable proper dataset selection for AI Model
Training.</p>
<p><strong>Version 1.1.0</strong></p>
<p><strong><a
href="https://dataandtrustalliance.org/DPS">Published</a></strong>
Draft</p>
<p><strong>Authors:</strong></p>
<ul>
<li><a href="https://dataandtrustalliance.org/">Data and Trust
Alliance</a></li>
<li>Kristina Podnar</li>
<li>John Moehrke</li>
</ul>
<p><strong>Comments</strong> <a
href="mailto:inquiries@dataandtrustalliance.org">inquiries@dataandtrustalliance.org</a></p>
<p><strong>License</strong> <a
href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
Attribution-ShareAlike 4.0 International</a></p>
<hr />
<h1 id="1-abstract">1. Abstract</h1>
<p>Data transparency is critical. Trust in the insights and decisions
coming from both traditional data and AI applications depends on
understanding the origin, lineage, and rights associated with the data
that feeds them. Lack of transparency has real costs, including
unnecessary risks and foregone opportunities. And yet, many
organizations today cannot answer basic data questions without
considerable difficulty and investment.</p>
<p>To realize the value of data and AI requires a reliable
cross-industry baseline of data transparency. The Data Provenance
Standard proposes a solution.</p>
<p><strong>Table of Content</strong></p>
<ol type="1">
<li><a href="#1-abstract">Abstract</a></li>
<li><a href="#2-introduction">Introduction</a></li>
<li><a href="#3-use-case">Use-Cases</a></li>
<li><a href="#4-abstract-specification">Abstract Specification</a></li>
<li><a href="#5-technical-encoding">Technical Encoding</a></li>
<li><a href="#6-security-considerations">Security
Considerations</a></li>
<li><a href="#7-annex">Annex</a></li>
<li><a href="#8-acknowledgements">Acknowledgements</a></li>
</ol>
<h1 id="2-introduction">2. Introduction</h1>
<p>For the past quarter century, companies have worked to become
"digital". Now they are using data, algorithms, and AI to reinvent how
decisions are made. These companies are becoming "data enterprises".
This transition is transforming industry sectors from retail, education,
and entertainment to health and wellness, transportation, and
energy.</p>
<p>For these intelligent systems to create economic and societal value
for all stakeholders—customers, employees, citizens, shareholders,
partners, suppliers, and more—they must be designed and managed
responsibly.</p>
<p>The Data Provenance Standard surface metadata on source, legal
rights, privacy &amp; protection, generation date, data type, generation
method, intended use, restrictions, and lineage. Each metadata field has
associated values. This essential information about the origin of and
rights associated with data allows enterprises to make informed choices
about the data they source and use. The result can be improvements in
operational efficiency, regulatory compliance, collaboration, and value
generation.</p>
<h2 id="21-the-data--trust-alliance">2.1 The Data &amp; Trust
Alliance</h2>
<p>The Data &amp; Trust Alliance brings together leading businesses and
institutions across multiple industries to learn, develop, and adopt
responsible data and AI practices. Established in September 2020, it was
founded as a not-for-profit consortium and is co-chaired by Ken
Chenault, General Catalyst chairman and managing director, and former
American Express chairman and CEO; and Sam Palmisano, former IBM
chairman and CEO. Jon Iwata, founding executive director, works with our
Leadership Council, a cross-functional team of senior executives
selected by their CEOs to identify and drive Alliance projects.</p>
<h2 id="22-normative-vs-informative">2.2 Normative vs Informative</h2>
<p>This specification adopts the normative words defined in IETF <a
href="https://www.rfc-editor.org/info/bcp14">Best Current Practice
14</a>: Key words for use in RFCs to Indicate Requirement Levels
(BCP-14), certain words indicate whether a specific content is
normative. The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL
NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in
this document are to be interpreted as described. Informative content
does not contain these key words.</p>
<h2 id="23-maintenance-of-the-specification">2.3 Maintenance of the
specification</h2>
<p>The D&amp;TA Data Provenance Standard Working Group is responsible
for the ongoing monitoring and periodic review of the terms in the
registry. This includes:</p>
<ul>
<li>Annual review of all terms to ensure continued relevance</li>
<li>Soliciting feedback from the community on the usage and
applicability of terms</li>
<li>Making revisions to definitions as needed</li>
<li>Retiring terms that are no longer relevant or have become
obsolete</li>
</ul>
<p>Management of D&amp;TA’s Data Provenance Standard vocabulary registry
is a collaborative and dynamic process. We welcome feedback from the
broader community to ensure that the vocabulary remains a valuable and
accurate resource for all.</p>
<h2 id="24-glossary">2.4 Glossary</h2>
<p>The following concepts are used throughout the specification</p>
<ul>
<li><strong>AI</strong>: Computer systems that can perform tasks
typically requiring human intelligence, such as understanding language,
recognizing patterns, and making decisions.</li>
<li><strong>Dataset</strong>: A collection of data.</li>
<li><strong>Metadata</strong>: Structured description about a dataset
that enable the understanding of that dataset.</li>
<li><strong>Dataset Identifier</strong>: A unique label identifying the
provenance metadata of the current dataset</li>
<li><strong>Lineage</strong>: Identifiers or pointers of metadata
representing the data which comprise the current dataset</li>
<li><strong>Source</strong>: Identifies the origin (person,
organization, system, device, etc.) of the current dataset</li>
<li><strong>Legal Rights</strong>: Identifies the lawful entitlements
and protections over data ownership, use, and distribution, ensuring
compliance, privacy, and accountability</li>
<li><strong>Privacy and Protection</strong>: Identifies any types of
sensitive data associated with the current dataset and any privacy
enhancing techniques applied</li>
<li><strong>Generation Date</strong>: Timestamp marking the creation of
the current dataset</li>
<li><strong>Data Type</strong>: Identifies the data type contained in
the current set, and provides insights into how the data is organized,
its potential use cases, and the challenges associated with handling and
using it</li>
<li><strong>Generation Method</strong>: Identifies how the data was
produced (data mining, machine-generated, IoT sensors, etc.)</li>
<li><strong>Intended Use and Restrictions</strong>: Identifies the
intended use of the data and which downstream audiences should not be
allowed access to the current dataset</li>
</ul>
<h1 id="3-use-case">3. Use-Case</h1>
<p>The following use-cases are examplar of the use-cases for which this
standard applies. These use-cases show how this standard supports
diverse needs across the data ecosystem.</p>
<ol type="1">
<li>Healthcare insurance data procurement</li>
<li>Media consumption pattern data set for consumer behavior
insights</li>
<li>Financial services customer product enablement</li>
<li>Enhancing global logistics efficiency through AI-driven tariff
harmonization</li>
</ol>
<h2 id="31-use-case-1-healthcare-insurance-data-procurement">3.1
Use-Case 1: Healthcare insurance data procurement</h2>
<p>Bella and her team are evaluating a new dataset that contains
comprehensive patient and insurance payment information. This dataset is
considered crucial for enhancing the company’s predictive analytics
models which forecast healthcare trends, personalize insurance plans,
and optimize claim processing.</p>
<p>Goals</p>
<ul>
<li>Establish lineage and align with data provenance insights.</li>
<li>Comply with healthcare regulations, focusing on confidentiality,
consent, and appropriate data handling.</li>
<li>Enhance operational efficiency and analytical models without major
disruptions.</li>
<li>Leverage the dataset for developing innovative strategies and
improving customer trust by analyzing the dataset’s intent and
proprietary content for new personalized engagement opportunities.</li>
</ul>
<p>Challenges</p>
<ul>
<li>Balancing the need for detailed, comprehensive data with privacy and
confidentiality requirements.</li>
<li>Ensuring the dataset’s metadata is accurate, up-to-date, and
compliant with evolving Data Provenance Standard.</li>
<li>Integrating new datasets with existing systems and models without
compromising data integrity or system performance.</li>
<li>Navigating the complex landscape of healthcare regulations and
ensuring all data usage is compliant.</li>
</ul>
<h3 id="311-use-case-1-needs">3.1.1 Use-Case 1 needs</h3>
<ul>
<li>Has the data already been incorporated?</li>
<li>Who has generated the data? Are they a trusted supplier?</li>
<li>Where was the data captured from?</li>
<li>When was the data captured, and how often is it updated?</li>
<li>How was the data captured?</li>
<li>What format is the dataset?</li>
<li>Were privacy enhancing technologies (PETs) or tools applied to the
dataset in order to remove, mask, or modify PI/SPI in the data?</li>
<li>What kind of privacy classes of data are in the dataset?</li>
<li>Are there appropriate consents covering the dataset?</li>
<li>Are there geographical restrictions on processing or storage?</li>
<li>What license, copyright, trademark, or patent requirements?</li>
<li>What are the allowances or restrictions on the kinds of use (e.g. AI
training)?</li>
</ul>
<h3 id="312-outcome">3.1.2 Outcome</h3>
<p>Through comprehensive metadata evaluation, the procurement team was
able to perform a more in-depth assessment of a crucial dataset. The
results were decreasing procurement time and risk while enhancing the
company’s predictive analytics capabilities and maintaining adherence to
legal and ethical standards. The benefits of having access to the
dataset metadata include:</p>
<ul>
<li>Better dataset evaluation.</li>
<li>Title, unique identifier, and details about data origin and
collection methods allowed for quick insights into the dataset’s
provenance without having to read through many pages of descriptive
text.</li>
<li>Confidentiality, consent documentation, data processing, and storage
geographies provided evidence of the dataset’s compliance with
healthcare regulations.</li>
<li>Clarity from the metadata enabled coordination and discussions
between the procurement and data teams to assess the dataset’s impact on
analytical models, ensuring seamless integration and operational
improvement.</li>
<li>Increased risk and opportunity balance.</li>
<li>Metadata around generation method and permitted use proved critical
to the organization’s innovative marketing strategies and improving
internal customer trust of the acquired data.</li>
<li>Collaboration between the legal and compliance departments was
boosted because it was clear at the onset who needed to be involved in
which aspects of dataset acquisition evaluation, ensuring the dataset’s
use was in strict adherence to healthcare regulations and company
policies.</li>
<li>Improved data procurement and legal validation.</li>
<li>Suspect data processing and storage metadata resulted in increased
legal scrutiny in advance of data acquisition, leading to the correction
of inaccuracies in the metadata.</li>
<li>The metadata evaluation process, despite causing a brief delay,
ultimately safeguarded the organization against potential legal and
operational risks, ensuring the dataset’s strategic and compliant
use.</li>
</ul>
<h2
id="32-use-case-2-media-consumption-pattern-data-set-for-consumer-behavior-insights">3.2
Use-Case 2: Media consumption pattern data set for consumer behavior
insights</h2>
<p>Curating a high-quality dataset that tracks media consumption habits
across diverse platforms for content personalization.</p>
<p>Jordan’s current project involves curating a dataset that tracks
media consumption habits across diverse platforms. This dataset aims to
empower media buyers and sellers in accurately targeting their audience
segments, facilitating personalized content strategies for industries
ranging from consumer goods to tourism.</p>
<p>Goals</p>
<ul>
<li>Ensure comprehensive coverage of media consumption patterns to
provide actionable insights for diverse industries.</li>
<li>Maintain high standards of data transparency to build trust and
encourage collaboration.</li>
<li>Enhance clients’ operational efficiency and compliance through
strategic data integration.</li>
</ul>
<p>Challenges</p>
<ul>
<li>Balancing data comprehensiveness with privacy and ethical
considerations.</li>
<li>Keeping pace with rapid changes in media consumption behaviors and
technology.</li>
<li>Ensuring data standards provide necessary transparency to data
buyers and that the metadata is compatible with automated data
procurement systems.</li>
</ul>
<h3 id="321-use-case-2-needs">3.2.1 Use-Case 2 needs</h3>
<p>In addition to some needs outlined in Use-Case 1:</p>
<ul>
<li>Traceability to all data sources</li>
<li>Applicability of the dataset to the use-case</li>
<li>Dataset covers the geography of interest</li>
<li>Dataset has covered full timeframe of interest</li>
<li>Are there restrictions on the use?</li>
<li>Can the data be used for AI, educational, and developmental
programs?</li>
</ul>
<h3 id="322-outcome">3.2.2 Outcome</h3>
<p>Metadata associated with the "March 2024 Global Media Consumption
Trends" dataset is a vital resource for procuring complex media
consumption patterns, ensuring its integrity and applicability in AI
analytics. This approach to describing data can facilitate effective
personalization of content strategies across various industries and will
set a new standard for transparent, efficient, and compliant data usage
in media consumption analysis. The outcome includes:</p>
<ul>
<li>Comprehensive coverage and actionable insights
<ul>
<li>Detailed data origin geography and collection methodologies will
assure users of the data’s relevance and quality, thus facilitating
targeted content strategies.</li>
</ul></li>
<li>Transparency and trust building
<ul>
<li>By adhering to the Data Provenance Standard and providing a clear
metadata URL, the dataset’s transparency is increased, making it easier
for media buyers and sellers to assess its credibility.</li>
<li>The documentation of dataset lineage and the use of PETs-like
anonymization underscores the commitment to data privacy and security,
building trust among clients.</li>
</ul></li>
<li>Increased efficiency and enhanced compliance
<ul>
<li>The dataset’s lack of proprietary data restrictions and the
provision of a clear license to use, as indicated by contacting
AnalytiQuest Ventures’ Office of General Counsel, streamlines the data
acquisition process, enhancing clients’ operational efficiency.</li>
<li>Data processing and storage geography metadata will help downstream
consumers comply with legal standards and privacy regulations, thereby
reducing legal and reputational risks.</li>
</ul></li>
</ul>
<h2
id="33-use-case-3-financial-services-customer-product-enablement">3.3
Use-Case 3: Financial services customer product enablement</h2>
<p>Evaluating a new dataset for refining AI algorithms used in customer
credit card offerings.</p>
<p>Minh is tasked with evaluating a new dataset for refining AI
algorithms for customer credit card offerings. The dataset under
consideration has been documented in accordance with the Data Provenance
Standard, ensuring transparency and compliance, especially under GDPR
and the new EU AI Act. Minh’s evaluation process focuses on the detailed
metadata provided for the dataset.</p>
<p>Goals</p>
<ul>
<li>Enhance AI model accuracy for customized credit card offerings.</li>
<li>Ensure compliance with international laws like GDPR to mitigate
legal risks.</li>
<li>Optimize data handling practices within geographical limits for
increased efficiency.</li>
<li>Maintain thorough documentation for dataset transparency and
accountability.</li>
<li>Uphold dataset quality and integrity to bolster trust in AI
insights.</li>
</ul>
<p>Challenges</p>
<ul>
<li>Ensure dataset credibility through clear documentation of its
lineage and metadata.</li>
<li>Navigate diverse international regulations related to data privacy
and AI.</li>
<li>Integrate the new dataset with existing systems without operational
disruptions.</li>
<li>Balance proprietary data use with information protection and
competitive edge.</li>
<li>Confirm dataset use is ethical and consensual, particularly with
sensitive data.</li>
<li>Keep pace with technological and data standard advancements for AI
relevance.</li>
</ul>
<h3 id="331-use-case-3-needs">3.3.1 Use-Case 3 needs</h3>
<p>In addition to some needs outlined in Use-Case 1 and 2:</p>
<ul>
<li>Has the dataset been anonymized compliant to privacy
regulations?</li>
<li>Are there any publication restrictions or limits?</li>
</ul>
<h3 id="332-outcome">3.3.2 Outcome</h3>
<p>Minh’s evaluation of the "Consumer Spending Patterns 2020-2024"
dataset through the lens of the Data Provenance Standard significantly
enhanced ProForma Financial Services’ AI algorithms for customer credit
card offerings, ensuring both heightened personalization and strict
adherence to international data regulations. This approach improved the
precision and effectiveness of the company’s AI models and ensured
compliance, data privacy, and seamless integration with existing
systems, paving the way for responsible and innovative use of AI
insights in the financial sector. The outcome includes:</p>
<ul>
<li>Better business case alignment
<ul>
<li>By not relying solely on high-level dataset descriptions, this
process elevated the focus on standards and metadata for business case
alignment and ultimate success.</li>
<li>The provided origin, creation dates, and collection method of the
metadata assured that the dataset relevance and potential quality were
acceptable.</li>
</ul></li>
<li>Faster data acquisition timeline and speed to market
<ul>
<li>The absence of proprietary data restrictions and clear licensing
terms sped up ProForma’s data acquisition process and ability to develop
AI capabilities.</li>
</ul></li>
<li>Increased compliance and integrity
<ul>
<li>The ability to assess dataset compliance with the latest Data
Provenance Standard, including versioning and unique identifiers meant
that the initial assessment of fit-for-use could be performed by an
automated system and passed to a human for deeper review.</li>
<li>The dataset lineage, original sources, and PETs allowed ProForma to
quickly gauge the level of pre-processing necessary to comply with data
privacy requirements.</li>
<li>The data collection, processing and storage information further
helped meet data privacy requirements and mitigate legal and
reputational risks associated with GDPR and the EU AI Act.</li>
</ul></li>
</ul>
<h2
id="34-use-case-4-enhancing-global-logistics-efficiency-through-ai-driven-tariff-harmonization">3.4
Use-Case 4: Enhancing global logistics efficiency through AI-driven
tariff harmonization</h2>
<p>Managing data to refine AI systems for accurately predicting tariff
costs across countries and categories.</p>
<p>The global nature of Navisphere Logistics, Ltd.’s operations means
that the company must navigate a complex web of international tariffs
and customs regulations. Efficiently managing these tariffs is critical
to minimizing delivery times and costs. Dr. Hicks and her team are
tasked with refining the company’s AI systems to accurately predict
tariff costs across different countries and product categories.</p>
<p>Goals</p>
<ul>
<li>Unify global tariff schedules into an AI-compatible format for
better predictions.</li>
<li>Enhance AI tariff models to reduce cross-border delivery times and
costs.</li>
<li>Adhere to Data Provenance Standard for tariff data integrity and
compliance.</li>
<li>Attain tariff predictions globally across various product categories
with advanced AI.</li>
<li>Simplify customs processes with accurate tariff assessments, aiding
global clients.</li>
</ul>
<p>Challenges</p>
<ul>
<li>Navigate the intricate international tariff and customs landscape
with diverse rules.</li>
<li>Rigorously assess dataset metadata for compliance with global
standards and privacy.</li>
<li>Continually update AI models to adapt to changing international
tariff regulations.</li>
<li>Balance advanced AI development with responsible usage and adherence
to privacy laws.</li>
<li>Ensure smooth AI model integration into Navisphere Logistics’
systems without workflow disruption.</li>
</ul>
<h3 id="341-use-case-4-needs">3.4.1 Use-Case 4 needs</h3>
<p>In addition to some needs outlined in Use-Case 1, 2, and 3:</p>
<ul>
<li>Indications of data credibility or bias.</li>
<li>Indications that the data are original vs intermediary combined from
multiple sources</li>
</ul>
<h3 id="342-outcome">3.4.2 Outcome</h3>
<p>Through application of the Data Provenance Standard metadata for its
global tariff schedule datasets, Navisphere Logistics, Ltd., has
achieved a significant enhancement in the operational efficiency and
accuracy of its AI-driven tariff prediction models. The outcome
includes:</p>
<ul>
<li>Improved data consistency and compatibility
<ul>
<li>By specifying the version used for the metadata, Navisphere ensured
that all datasets adhered to a uniform standard, facilitating seamless
integration and interpretation by the AI models, regardless of the
data’s origin or when it was collected.</li>
</ul></li>
<li>Enhanced data identification and access
<ul>
<li>The establishment of a unique metadata identifier and a metadata
unique URL for each dataset enabled easy identification, access, and
reference; streamlining the data ingestion process for the AI systems;
and reduced the time spent on data preprocessing.</li>
</ul></li>
<li>Streamlined lineage and dependency tracking
<ul>
<li>The metadata location for datasets feeding the current dataset
allowed Navisphere to efficiently manage data dependencies and lineage,
ensuring that updates or corrections in source datasets could be rapidly
propagated through the system, maintaining the accuracy and timeliness
of tariff predictions.</li>
</ul></li>
<li>Increased accountability and data integrity
<ul>
<li>Detailed metadata entries for the creator, source, and data origin
geography provided clear accountability and context for the data,
enhancing trust in the data’s reliability and compliance with regional
laws and international regulations.</li>
</ul></li>
<li>Better data privacy and security measures
<ul>
<li>The application of privacy enhancing technologies (PETs) and the
careful classification of data confidentiality ensured that personally
identifiable information (PII) and sensitive personal information (SPI)
were adequately protected, aligning with global privacy standards and
ethical considerations in AI application.</li>
</ul></li>
<li>Legal compliance
<ul>
<li>Detailed metadata on data processing and storage geographies,
consent locations, and the license to use the data ensured that all AI
operations remained within legal boundaries, respecting data sovereignty
laws and consent agreements.</li>
</ul></li>
</ul>
<h1 id="4-abstract-specification">4. Abstract Specification</h1>
<p>The Data Provenance Standard is made up of three groups of metadata
elements: Source, Provenance, and Use.</p>
<p><img src="./out/ImageSource/metadata/metadata.svg"
alt="Metadata Structure" /></p>
<h2 id="41-source">4.1 Source</h2>
<p>This group describes the dataset and the source of the dataset.</p>
<ul>
<li>Element-Name: <code>source</code></li>
<li>cardinality: 1..1</li>
<li>The following are child elements</li>
</ul>
<h3 id="411-standards-version-used">4.1.1 Standards version used</h3>
<p>Specifies the version of the schema or standards used to define the
metadata for this dataset, ensuring consistency and compatibility over
time.</p>
<ul>
<li>Element-Name: <code>version</code></li>
<li>cardinality: 1..1</li>
<li>Format: String, Prefer Semantic Versioning (a.k.a., SemVer) format -
<a href="https://semver.org/">https://semver.org/</a></li>
<li>Example: <code>1.1.0</code></li>
</ul>
<h3 id="412-dataset-title--name">4.1.2 Dataset title / name</h3>
<p>The official name of the dataset, which should be descriptive and
help easily identify the dataset's content and purpose.</p>
<ul>
<li>Element-Name: <code>title</code></li>
<li>cardinality: 1..1</li>
<li>Format: String</li>
<li>Example: <code>Blue sky observations</code></li>
</ul>
<h3 id="413-unique-metadata-identifier">4.1.3 Unique metadata
identifier</h3>
<p>A distinct identifier (such as a UUID) assigned to the dataset's
metadata to uniquely distinguish it from others, ensuring no confusion
or overlap.</p>
<ul>
<li>Element-Name: <code>id</code></li>
<li>cardinality: 1..1</li>
<li>Format: URI</li>
<li>Example:
<code>urn:uuid:17725bad-9098-4f43-abe6-43490ae1596c</code></li>
</ul>
<h3 id="414-metadata-location">4.1.4 Metadata location</h3>
<p>The web address where the dataset's metadata is published and can be
accessed, providing a direct link to detailed information about the
dataset. Typically will be a unique URL of the current dataset</p>
<ul>
<li>Element-Name: <code>location</code></li>
<li>cardinality: 0..1</li>
<li>Format: URL</li>
<li>Example: <code>https://example.org</code></li>
</ul>
<h3 id="415-dataset-issuer">4.1.5 Dataset issuer</h3>
<p>The legal entity responsible for creating the dataset, providing
accountability and a point of contact for inquiries.</p>
<ul>
<li>Element-Name: <code>issuer</code></li>
<li>cardinality: 1..*</li>
<li>Format: Organization</li>
</ul>
<h3 id="416-description-of-the-dataset">4.1.6 Description of the
dataset</h3>
<p>Contains a detailed narrative that explains the contents, scope, and
purpose of the dataset. It provides essential contextual information
that helps users understand what the data represents, how it was
collected, and any limitations or recommended uses.</p>
<ul>
<li>Element-Name: <code>description</code></li>
<li>cardinality: 1..1</li>
<li>Format: Markdown</li>
</ul>
<h2 id="42-provenance">4.2 Provenance</h2>
<p>This group describes the provenance of the dataset</p>
<ul>
<li>Element-Name: <code>provenance</code></li>
<li>cardinality: 1..1</li>
<li>The following are child elements</li>
</ul>
<h3 id="421-source-metadata-for-dataset">4.2.1 Source metadata for
dataset</h3>
<p>Identifies where the metadata for any source datasets that contribute
to the current dataset can be found, establishing lineage and
dependencies. This field establishes lineage.</p>
<ul>
<li>Element-Name: <code>source</code></li>
<li>cardinality: 0..1</li>
<li>Format: URL</li>
<li>Example: <code>https://example.org/dataset/blue.xml</code></li>
</ul>
<h3 id="422-source-for-dataset">4.2.2 Source for dataset</h3>
<p>If the data originates from a different organization than the one who
issued the dataset, this field identifies the original source's legal
name.</p>
<ul>
<li>Element-Name: <code>origin</code></li>
<li>cardinality: 0..1</li>
<li>Format: <a href="#451-organization">Organization</a></li>
</ul>
<h3 id="423-data-origin-geography">4.2.3 Data origin geography</h3>
<p>The geographical location where the data was originally collected,
which can be important for compliance with regional laws and
understanding the data's context.</p>
<ul>
<li>Element-Name: <code>origin-geography</code></li>
<li>cardinality: 1..*</li>
<li>Format: <a href="#452-location">Location</a></li>
</ul>
<h3 id="424-dataset-issue-date">4.2.4 Dataset issue date</h3>
<p>The date when the dataset was compiled or created, providing a
temporal context for the data.</p>
<ul>
<li>Element-Name: <code>date</code></li>
<li>cardinality: 1..1</li>
<li>Format: DateTime</li>
<li>Example: <code>2024-05-27</code>, or
<code>2024-05-27T15:18:02Z</code></li>
</ul>
<h3 id="425-date-of-previously-issued-version-of-the-dataset">4.2.5 Date
of previously issued version of the dataset</h3>
<p>The release date of the last version of the dataset, if it has been
updated or revised, to track changes and updates over time.</p>
<ul>
<li>Element-Name: <code>previous-date</code></li>
<li>cardinality: 0..1</li>
<li>Format: DateTime</li>
<li>Example: <code>2024-05-27</code>, or
<code>2024-05-27T15:18:02Z</code></li>
</ul>
<h3 id="426-range-of-dates-for-data-generation">4.2.6 Range of dates for
data generation</h3>
<p>The span of time during which the data within the dataset was
collected or generated, offering insight into the dataset's timeliness
and relevance.</p>
<ul>
<li>Element-Name: <code>generation-period</code></li>
<li>cardinality: 0..1</li>
<li>Format: <a href="#453-period">Period</a></li>
</ul>
<h3 id="427-method">4.2.7 Method</h3>
<p>The methodology or procedures used to collect, generate, or compile
the data, giving insight into its reliability and validity.</p>
<ul>
<li>Element-Name: <code>generation-method</code></li>
<li>cardinality: 1..*</li>
<li>Format: <a href="#454-concept">Concept</a>
<ul>
<li>SHALL be from <a href="#71-data-collection-method-codes">Specific
method types</a> if applicable</li>
</ul></li>
<li>MAY be other code from other code systems</li>
</ul>
<h3 id="428-data-format">4.2.8 Data format</h3>
<p>Describes the nature of the data within the dataset, such as
numerical, textual, or multimedia, helping users understand what kind of
information is contained within the file or dataset.,</p>
<ul>
<li>Element-Name: <code>format</code></li>
<li>cardinality: 0..*</li>
<li>Format: String
<ul>
<li>SHALL be mime-type <a
href="https://www.rfc-editor.org/rfc/rfc6838.html">https://www.rfc-editor.org/rfc/rfc6838.html</a></li>
<li>SHALL be from registered media-types <a
href="http://www.iana.org/assignments/media-types/">http://www.iana.org/assignments/media-types/</a>
if applicable</li>
</ul></li>
<li>Example: <code>application/json</code></li>
</ul>
<h2 id="43-use">4.3 Use</h2>
<p>This group describes legal, use, and restrictions.</p>
<ul>
<li>Element-Name: <code>use</code></li>
<li>cardinality: 1..1</li>
<li>The following are child elements</li>
</ul>
<h3 id="431-confidentiality-classification">4.3.1 Confidentiality
classification</h3>
<p>Indicate if the dataset includes data falling into the
confidentiality classification. Each classifier must be evaluated as
true/false/unknown.</p>
<ul>
<li>Element-Name: <code>classification</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#455-classification">Classification</a></li>
<li>For each <a href="#74-data-regulation-codes">Regulation Domain</a>
that has been evaluated</li>
<li>Additional entries can be added</li>
</ul>
<h3 id="432-consent-documentation-location">4.3.2 Consent documentation
location</h3>
<p>Specifies where consent documentation or agreements related to the
data can be found, ensuring legal compliance and regulatory use. This
element must be populated when Privacy Consent is appropriate. When
populated it points to either one Privacy Consent Policy that all
individuals in the dataset agreed to, or one Privacy Policy for each
individual in the dataset with that individual's signature.</p>
<ul>
<li>Element-Name: <code>consents</code></li>
<li>cardinality: 0..*</li>
<li>Format: URL</li>
<li>Example:
<code>https://example.org/dataset/34/consent.pdf</code></li>
</ul>
<h3 id="433-privacy-indicators">4.3.3 Privacy indicators</h3>
<p>Indicates whether techniques were used to protect personally
identifiable information (PII) or sensitive personal information (SPI),
highlighting the dataset's privacy considerations.</p>
<ul>
<li>Element-Name: <code>privacy-indicators</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#433-privacy-indicators">PrivacyIndicator</a></li>
<li>using <a href="#72-privacy-enhancing-tools-codes">Privacy Enhancing
Tools</a> vocabulary</li>
</ul>
<h3 id="434-data-processing-geography-included">4.3.4 Data processing
geography included</h3>
<p>Defines the geographical boundaries within which the data can be
processed, often for legal or regulatory reasons.</p>
<ul>
<li>Element-Name: <code>processing-included</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#452-location">Location</a></li>
<li>When not populated there are no processing location
requirements</li>
</ul>
<h3 id="435-data-processing-geography-excluded">4.3.5 Data processing
geography excluded</h3>
<p>Defines the geographical boundaries within which the data cannot be
processed, often for legal or regulatory reasons.</p>
<ul>
<li>Element-Name: <code>processing-excluded</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#452-location">Location</a></li>
<li>When not populated there are no processing location
restrictions</li>
</ul>
<h3 id="436-data-storage-geography-allowed">4.3.6 Data storage geography
allowed</h3>
<p>Specifies where the data may be stored, crucial for compliance with
data sovereignty laws.</p>
<ul>
<li>Element-Name: <code>storage-allowed</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#452-location">Location</a></li>
<li>When not populated there are no defined storage location
requirements</li>
</ul>
<h3 id="437-data-storage-geography-forbidden">4.3.7 Data storage
geography forbidden</h3>
<p>Specifies where the data may not be stored, crucial for compliance
with data sovereignty laws.</p>
<ul>
<li>Element-Name: <code>storage-forbidden</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#452-location">Location</a></li>
<li>When not populated there are no defined storage location
restrictions</li>
</ul>
<h3 id="438-license-to-use">4.3.8 License to use</h3>
<p>Details the location or point of contact for identifying the terms
under which the dataset can be used, including any restrictions or
obligations, clarifying legal use and distribution rights. License may
be an End User License Agreement (EULA), subject to Data Use Agreement
(DUA).</p>
<ul>
<li>Element-Name: <code>license</code></li>
<li>cardinality: 0..1</li>
<li>Format: String
<ul>
<li>Prefer License codes such as CreativeCommons or Apache</li>
<li>May be a URL</li>
</ul></li>
<li>Example: <code>Apache-2.0</code></li>
</ul>
<h3 id="439-intended-data-use">4.3.9 Intended data use</h3>
<p>Describes the purpose for which the dataset was created, guiding
users on its intended use and potential applications against identified
use cases. List all that apply from the <a
href="#73-data-use-codes">Data Use</a> codes that apply. Additional
codes can be included with descriptions.</p>
<ul>
<li>Element-Name: <code>intended-purpose</code></li>
<li>cardinality: 0..*</li>
<li>Format: <a href="#454-concept">Concept</a>
<ul>
<li>SHALL populate <code>code</code> from <a
href="#73-data-use-codes">Data Use</a>.</li>
<li>SHALL populate <code>description</code> with specific
description</li>
<li>When using the code <code>non-ai-other</code> and
<code>ai-other</code>, the description SHALL describe the actual
use</li>
</ul></li>
</ul>
<h3 id="4310-copyright">4.3.10 Copyright</h3>
<p>Indicates whether the dataset contains proprietary information that
is covered with a Copyright and the terms of said Copyright.</p>
<ul>
<li>Element-Name: <code>copyright</code></li>
<li>cardinality: 0..*</li>
<li>Format: String
<ul>
<li>where the string <code>no</code> indicates no Copyright</li>
</ul></li>
</ul>
<h3 id="4311-patent">4.3.11 Patent</h3>
<p>Indicates whether the dataset contains proprietary information that
is covered with a Patent and said Patent number.</p>
<ul>
<li>Element-Name: <code>patent</code></li>
<li>cardinality: 0..*</li>
<li>Format: String
<ul>
<li>where the string <code>no</code> indicates no Patent</li>
</ul></li>
</ul>
<h3 id="4312-trademark">4.3.12 Trademark</h3>
<p>Indicates whether the dataset contains proprietary information that
is covered with a Trademark, and the terms of said Trademark.</p>
<ul>
<li>Element-Name: <code>trademark</code></li>
<li>cardinality: 0..*</li>
<li>Format: String
<ul>
<li>where the string <code>no</code> indicates no Trademark</li>
</ul></li>
</ul>
<h2 id="44-primitive-datatypes">4.4 Primitive Datatypes</h2>
<p>These datatypes are commonly understood.</p>
<ul>
<li><strong>String</strong> - A sequence of Unicode characters</li>
<li><strong>URI</strong> - A Uniform Resource Identifier Reference <a
href="https://www.rfc-editor.org/rfc/rfc3986">RFC 3986</a>. Note: URIs
are case sensitive. For UUID
(urn:uuid:53fefa32-fcbb-4ff8-8a92-55ee120877b7) use all lowercase</li>
<li><strong>URL</strong> - A Uniform Resource Locator <a
href="https://datatracker.ietf.org/doc/html/rfc1738">RFC 1738</a>. Note
URLs are accessed directly using the specified protocol. Common URL
protocols are http{s}:, ftp:, mailto: and mllp:, though many others are
defined</li>
<li><strong>Markdown</strong> - A String that can include markdown. This
specification requires and uses the <a
href="https://github.github.com/gfm/">GFM (Github Flavored Markdown)</a>
extensions on <a href="http://spec.commonmark.org/0.28/">CommonMark</a>
format, with the exception of support for inline HTML which is not
supported.</li>
<li><strong>dateTime</strong> A date, date-time or partial date (e.g.
just year or year + month) as used in human communication. The format is
a subset of <a
href="https://www.iso.org/iso-8601-date-and-time-format.html">ISO8601</a>
icon: YYYY, YYYY-MM, YYYY-MM-DD or YYYY-MM-DDThh:mm:ss+zz:zz, e.g. 2018,
1973-06, 1905-08-23, 2015-02-07T13:28:17-05:00 or
2017-01-01T00:00:00.000Z. If hours and minutes are specified, a timezone
offset SHALL be populated. Actual timezone codes can be sent using the
Timezone Code extension, if desired. Seconds must be provided due to
schema type constraints but may be zero-filled and may be ignored at
receiver discretion. Milliseconds are optionally allowed. Dates SHALL be
valid dates. The time "24:00" is not allowed. Leap Seconds are
allowed</li>
</ul>
<h2 id="45-complex-datatypes">4.5 Complex Datatypes</h2>
<p>These are made up of more than one child element as described.</p>
<p><img src="./out/ImageSource/datatypes/datatypes.svg"
alt="Datatype Structure" /></p>
<h3 id="451-organization">4.5.1 Organization</h3>
<ul>
<li>Legal Entity Name
<ul>
<li>Element-Name: <code>name</code></li>
<li>cardinality: 1..1</li>
<li>Format: String</li>
</ul></li>
<li>Legal entity Address
<ul>
<li>Element-Name: <code>address</code></li>
<li>cardinality: 0..*</li>
<li>Format: String</li>
</ul></li>
</ul>
<h3 id="452-location">4.5.2 Location</h3>
<ul>
<li>Country
<ul>
<li>Element-Name: <code>country</code>
<ul>
<li>cardinality: 1..1</li>
<li>Format: String
<ul>
<li>SHOULD be ISO 3166 2 letter, 3 letter code, or 3 digit country
codes</li>
</ul></li>
</ul></li>
</ul></li>
<li>State or Provenance
<ul>
<li>Element-Name: <code>state</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: String
<ul>
<li>SHOULD be ISO 3166-2: Codes for the names of the principal
subdivisions (e.g., states or provinces) of all countries coded in ISO
3166-1.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="453-period">4.5.3 Period</h3>
<ul>
<li>Start date/time
<ul>
<li>Element-Name: <code>start</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: DateTime</li>
<li>if empty, there is no start</li>
</ul></li>
</ul></li>
<li>End date/time
<ul>
<li>Element-Name: <code>end</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: DateTime</li>
<li>if empty, there is no end</li>
</ul></li>
</ul></li>
</ul>
<h3 id="454-concept">4.5.4 Concept</h3>
<ul>
<li>specific code for computable
<ul>
<li>Element-Name: <code>code</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: String
<ul>
<li>Should be from a given vocabulary</li>
<li>SHOULD be compute friendly, without spaces</li>
</ul></li>
</ul></li>
</ul></li>
<li>source of code
<ul>
<li>Element-Name: <code>system</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: URI</li>
</ul></li>
</ul></li>
<li>description of code for human
<ul>
<li>Element-Name: <code>description</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: String</li>
</ul></li>
</ul></li>
</ul>
<h3 id="455-classification">4.5.5 Classification</h3>
<ul>
<li>specific global regulation domain
<ul>
<li>Element-Name: <code>regulation</code>
<ul>
<li>cardinality: 1..1</li>
<li>Format: Concept
<ul>
<li>Should be from <a href="#74-data-regulation-codes">Data Regulation
Codes</a></li>
</ul></li>
</ul></li>
</ul></li>
<li>Has the dataset been evaluated to the regulation domain?
<ul>
<li>Element-Name: <code>evaluated</code>
<ul>
<li>cardinality: 1..1</li>
<li>Format: Boolean</li>
</ul></li>
</ul></li>
</ul>
<h3 id="456-privacyindicator">4.5.6 PrivacyIndicator</h3>
<ul>
<li>specific global regulation domain
<ul>
<li>Element-Name: <code>tool</code>
<ul>
<li>cardinality: 1..1</li>
<li>Format: Concept
<ul>
<li>Should be from <a href="#72-privacy-enhancing-tools-codes">Privacy
Enhancing Tools</a> vocabulary</li>
</ul></li>
</ul></li>
</ul></li>
<li>Parameters used with the tool
<ul>
<li>Element-Name: <code>parameters</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: String</li>
</ul></li>
</ul></li>
<li>Results of the tool use
<ul>
<li>Element-Name: <code>result</code>
<ul>
<li>cardinality: 0..1</li>
<li>Format: String</li>
</ul></li>
</ul></li>
</ul>
<h1 id="5-technical-encoding">5. Technical Encoding</h1>
<p>The detailed schema specification for JSON, XML, and YAML are
published independently in GIThub repositories. In these repositories
the following are to be found:</p>
<ul>
<li>schema - technical encoding of the rules</li>
<li>required vocabulary in that encoding format</li>
<li>examples</li>
</ul>
<h2 id="51-json-metadata-encoding">5.1 JSON Metadata Encoding</h2>
<p>The technical coding and examples are available in GIThub for the <a
href="https://github.com/Data-and-Trust-Alliance/json-metadata">JSON
metadata encoding</a></p>
<h2 id="52-xml-metadata-encoding">5.2 XML Metadata Encoding</h2>
<p>The technical coding and examples are available in GIThub for the <a
href="https://github.com/Data-and-Trust-Alliance/xml-metadata">XML
metadata encoding</a></p>
<h2 id="53-yaml-metadata-encoding">5.3 YAML Metadata Encoding</h2>
<p>The technical coding and examples are available in GIThub for the <a
href="https://github.com/Data-and-Trust-Alliance/yaml-metadata">YAML
metadata encoding</a></p>
<h1 id="6-security-considerations">6. Security considerations</h1>
<p>TODO: This section should advise the reader on security or privacy
things they should be aware of and for which a user of this
specification will need to consider. Often it is just a listing of risks
that the specification does not address, but for which the specification
creates. Such as the fact that datasets and metadata are sensitive data
and would need to be protected or carefully crafted such that they are
not a risk.</p>
<p>Metadata and the Dataset are data that need to be of quality and
trustable to be valuable.... blah blah</p>
<h1 id="7-annex">7. Annex</h1>
<p>The Annex contains normative vocabulary to be used when it applies.
This vocabulary SHALL be used when they apply.</p>
<h2 id="71-data-collection-method-codes">7.1 Data Collection Method
Codes</h2>
<p>The following defined vocabulary are to be used for the "Method".</p>
<ul>
<li>source URI:
<code>https://dataandtrustalliance.org/DPS/CodeSystem/Method</code></li>
</ul>
<table>
<thead>
<tr class="header">
<th>code</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>web-scraping-crawling-textual</td>
<td>Web scraping/Crawling Textual</td>
</tr>
<tr class="even">
<td>web-scraping-crawling-structured</td>
<td>Web scraping/Crawling Structured</td>
</tr>
<tr class="odd">
<td>web-scraping-crawling-metadata</td>
<td>Web scraping/Crawling Metadata</td>
</tr>
<tr class="even">
<td>web-scraping-crawling-social-media</td>
<td>Web scraping/Crawling Social media</td>
</tr>
<tr class="odd">
<td>web-scraping-crawling-news-articles</td>
<td>Web scraping/Crawling News &amp; articles</td>
</tr>
<tr class="even">
<td>web-scraping-crawling-other</td>
<td>Web scraping/Crawling Other</td>
</tr>
<tr class="odd">
<td>feeds-rss-source</td>
<td>Feeds RSS source</td>
</tr>
<tr class="even">
<td>feeds-api-source</td>
<td>Feeds API source</td>
</tr>
<tr class="odd">
<td>feeds-real-time-database-info</td>
<td>Feeds Real time database info</td>
</tr>
<tr class="even">
<td>feeds-interval-timed-database-info</td>
<td>Feeds Interval timed database info</td>
</tr>
<tr class="odd">
<td>feeds-file-feed-info</td>
<td>Feeds File feed info</td>
</tr>
<tr class="even">
<td>feeds-other</td>
<td>Feeds Other</td>
</tr>
<tr class="odd">
<td>syndication-news-feeds</td>
<td>Syndication News feeds</td>
</tr>
<tr class="even">
<td>syndication-financial-feeds</td>
<td>Syndication Financial feeds</td>
</tr>
<tr class="odd">
<td>syndication-social-media</td>
<td>Syndication Social media</td>
</tr>
<tr class="even">
<td>syndication-product-service-catalog</td>
<td>Syndication Product/service catalog</td>
</tr>
<tr class="odd">
<td>syndication-other</td>
<td>Syndication Other</td>
</tr>
<tr class="even">
<td>data-mining-association-rule</td>
<td>Data mining Association rule</td>
</tr>
<tr class="odd">
<td>data-mining-classification</td>
<td>Data mining Classification</td>
</tr>
<tr class="even">
<td>data-mining-clustering</td>
<td>Data mining Clustering</td>
</tr>
<tr class="odd">
<td>data-mining-regression</td>
<td>Data mining Regression</td>
</tr>
<tr class="even">
<td>data-mining-anomaly-detection</td>
<td>Data mining Anomaly detection</td>
</tr>
<tr class="odd">
<td>data-mining-sequencing</td>
<td>Data mining Sequencing</td>
</tr>
<tr class="even">
<td>data-mining-other</td>
<td>Data mining Other</td>
</tr>
<tr class="odd">
<td>machine-generated-mlops-synthetic</td>
<td>Machine generated/MLOps Synthetic</td>
</tr>
<tr class="even">
<td>machine-generated-mlops-generative</td>
<td>Machine generated/MLOps Generative</td>
</tr>
<tr class="odd">
<td>machine-generated-mlops-twin</td>
<td>Machine generated/MLOps Twin</td>
</tr>
<tr class="even">
<td>machine-generated-mlops-machine-2-machine</td>
<td>Machine generated/MLOps Machine-2-machine (M2M)</td>
</tr>
<tr class="odd">
<td>machine-generated-mlops-ai-inferred</td>
<td>Machine generated/MLOps AI inferred</td>
</tr>
<tr class="even">
<td>machine-generated-mlops-xr</td>
<td>Machine generated/MLOps XR (AR, VR, MR)</td>
</tr>
<tr class="odd">
<td>machine-generated-mlops-other</td>
<td>Machine generated/MLOps Other</td>
</tr>
<tr class="even">
<td>sensor-and-iot-output-environmental</td>
<td>Sensor and IoT output Environmental</td>
</tr>
<tr class="odd">
<td>sensor-and-iot-output-motion-location</td>
<td>Sensor and IoT output Motion &amp; location</td>
</tr>
<tr class="even">
<td>sensor-and-iot-output-health</td>
<td>Sensor and IoT output Health</td>
</tr>
<tr class="odd">
<td>sensor-and-iot-output-biometric</td>
<td>Sensor and IoT output Biometric</td>
</tr>
<tr class="even">
<td>sensor-and-iot-output-energy-consumption</td>
<td>Sensor and IoT output Energy consumption</td>
</tr>
<tr class="odd">
<td>sensor-and-iot-output-industrial-sensor</td>
<td>Sensor and IoT output Industrial sensor</td>
</tr>
<tr class="even">
<td>soensor-and-iot-output-vehicle-transportation</td>
<td>Sensor and IoT output Vehicle &amp; transportation</td>
</tr>
<tr class="odd">
<td>sensor-and-iot-output-security-surveillance</td>
<td>Sensor and IoT output Security &amp; surveillance</td>
</tr>
<tr class="even">
<td>sensor-and-iot-output-agriculture-environment</td>
<td>Sensor and IoT output Agriculture &amp; environment</td>
</tr>
<tr class="odd">
<td>sensor-and-iot-output-other</td>
<td>Sensor and IoT output Other</td>
</tr>
<tr class="even">
<td>social-media-text-based</td>
<td>Social media Text based</td>
</tr>
<tr class="odd">
<td>social-media-multimedia</td>
<td>Social media Multimedia</td>
</tr>
<tr class="even">
<td>social-media-reviews-and-ratings</td>
<td>Social media Reviews and ratings</td>
</tr>
<tr class="odd">
<td>social-media-updates</td>
<td>Social media Updates</td>
</tr>
<tr class="even">
<td>social-media-q-and-a</td>
<td>Social media Q&amp;As</td>
</tr>
<tr class="odd">
<td>social-media-collaborative</td>
<td>Social media Collaborative</td>
</tr>
<tr class="even">
<td>social-media-creative</td>
<td>Social media Creative</td>
</tr>
<tr class="odd">
<td>social-media-other</td>
<td>Social media Other</td>
</tr>
<tr class="even">
<td>user-generated-content-clickstream</td>
<td>User generated content Clickstream</td>
</tr>
<tr class="odd">
<td>user-generated-content-social-media</td>
<td>User generated content Social media</td>
</tr>
<tr class="even">
<td>user-generated-content-behavior</td>
<td>User generated content Behavior</td>
</tr>
<tr class="odd">
<td>user-generated-content-ratings-and-reviews</td>
<td>User generated content Ratings &amp; reviews</td>
</tr>
<tr class="even">
<td>user-generated-content-multimedia</td>
<td>User generated content Multimedia</td>
</tr>
<tr class="odd">
<td>user-generated-content-other</td>
<td>User generated content Other</td>
</tr>
<tr class="even">
<td>primary-user-source-survey-questionaire</td>
<td>Primary user source Survey/Questionnaire</td>
</tr>
<tr class="odd">
<td>primary-user-source-interview</td>
<td>Primary user source Interview</td>
</tr>
<tr class="even">
<td>primary-user-source-event</td>
<td>Primary user source Event</td>
</tr>
<tr class="odd">
<td>primary-user-source-biometric</td>
<td>Primary user source Biometric</td>
</tr>
<tr class="even">
<td>primary-user-source-focus-group</td>
<td>Primary user source Focus Group</td>
</tr>
<tr class="odd">
<td>primary-user-source-other</td>
<td>Primary user source Other</td>
</tr>
<tr class="even">
<td>data-augmentation-na</td>
<td>Data augmentation N/A</td>
</tr>
<tr class="odd">
<td>transfer-learning-na</td>
<td>Transfer learning N/A</td>
</tr>
<tr class="even">
<td>simulations-na</td>
<td>Simulations N/A</td>
</tr>
<tr class="odd">
<td>other-na</td>
<td>Other N/A</td>
</tr>
</tbody>
</table>
<h2 id="72-privacy-enhancing-tools-codes">7.2 Privacy Enhancing Tools
Codes</h2>
<p>The following concepts are defined to describe privacy enhancing
tools (PET).</p>
<ul>
<li>source URI:
<code>https://dataandtrustalliance.org/DPS/CodeSystem/PET</code></li>
</ul>
<table>
<thead>
<tr class="header">
<th>code</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>data-anonymization</td>
<td>Data Anonymization</td>
</tr>
<tr class="even">
<td>data-encryption</td>
<td>Data Encryption</td>
</tr>
<tr class="odd">
<td>data-masking</td>
<td>Data Masking</td>
</tr>
<tr class="even">
<td>data-minimization</td>
<td>Data Minimization</td>
</tr>
<tr class="odd">
<td>data-redaction</td>
<td>Data Redaction</td>
</tr>
<tr class="even">
<td>differential-privacy</td>
<td>Differential Privacy</td>
</tr>
<tr class="odd">
<td>federated-learning</td>
<td>Federated Learning</td>
</tr>
<tr class="even">
<td>homomorphic-encryption</td>
<td>Homomorphic Encryption</td>
</tr>
<tr class="odd">
<td>k-anonymity</td>
<td>K-anonymity</td>
</tr>
<tr class="even">
<td>l-diversity</td>
<td>L-diversity</td>
</tr>
<tr class="odd">
<td>pseudonymization</td>
<td>Pseudonymization</td>
</tr>
<tr class="even">
<td>secure-multi-party-computation</td>
<td>Secure Multi-party Computation (SMC)</td>
</tr>
<tr class="odd">
<td>t-closeness</td>
<td>T-closeness</td>
</tr>
<tr class="even">
<td>tokenization</td>
<td>Tokenization</td>
</tr>
</tbody>
</table>
<h2 id="73-data-use-codes">7.3 Data Use Codes</h2>
<p>The following concepts are defined to describe intended and forbidden
uses of the dataset.</p>
<ul>
<li>source URI:
<code>https://dataandtrustalliance.org/DPS/CodeSystem/Use</code></li>
</ul>
<table>
<thead>
<tr class="header">
<th>code</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>non-ai-staging</td>
<td>Non-AI Staging/testing</td>
</tr>
<tr class="even">
<td>non-ai-production</td>
<td>Non-AI Production</td>
</tr>
<tr class="odd">
<td>non-ai-quality</td>
<td>Non-AI Quality assurance</td>
</tr>
<tr class="even">
<td>non-ai-other</td>
<td>Non-AI Other</td>
</tr>
<tr class="odd">
<td>ai-pre-training</td>
<td>AI Pre-Training</td>
</tr>
<tr class="even">
<td>ai-alignment</td>
<td>AI Alignment</td>
</tr>
<tr class="odd">
<td>ai-evaluation</td>
<td>AI Evaluation</td>
</tr>
<tr class="even">
<td>ai-synthetic</td>
<td>AI Sythentic Data Generation</td>
</tr>
<tr class="odd">
<td>ai-other</td>
<td>AI Other</td>
</tr>
</tbody>
</table>
<p>Where: <code>non-ai-other</code>, and <code>ai-other</code></p>
<h2 id="74-data-regulation-codes">7.4 Data Regulation Codes</h2>
<p>The following concepts are defined to indicate specific global
regulated domains of relevance.</p>
<ul>
<li>source URI:
<code>https://dataandtrustalliance.org/DSP/CodeSystem/Regulations</code></li>
</ul>
<table>
<thead>
<tr class="header">
<th>code</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pi</td>
<td>Personal Information (PI)/Demographic</td>
</tr>
<tr class="even">
<td>pci</td>
<td>Payment Card Industry (PCI)</td>
</tr>
<tr class="odd">
<td>pfi</td>
<td>Personal Financial Information (PFI)</td>
</tr>
<tr class="even">
<td>pii</td>
<td>Personally Identifiable Information (PII)</td>
</tr>
<tr class="odd">
<td>phi</td>
<td>Personal Health Information (PHI)</td>
</tr>
<tr class="even">
<td>spi</td>
<td>Sensitive Personal Information (SPI)</td>
</tr>
</tbody>
</table>
<h1 id="8-acknowledgements">8. Acknowledgements</h1>
<p>Acknowledgements to the <a
href="https://dataandtrustalliance.org/who-we-are">Members of the Data
and Trust Alliance</a></p>
<p><strong>AARP</strong></p>
<ul>
<li>Amr Khani
<ul>
<li>SVP, Data and Analytics</li>
</ul></li>
<li>Jaye Campbell
<ul>
<li>Legal Leader, General Counsel</li>
</ul></li>
</ul>
<p><strong>American Express</strong></p>
<ul>
<li>Laurel Shifrin
<ul>
<li>VP, Enterprise Data Governance</li>
</ul></li>
<li>Saheel Shah
<ul>
<li>Director, Enterprise Data Governance</li>
</ul></li>
</ul>
<p><strong>CVS</strong></p>
<ul>
<li>Dave Sturgeon
<ul>
<li>Executive Director, Data Governance and Enablement</li>
</ul></li>
<li>Herb Holmes
<ul>
<li>Data Governance &amp; Data Management Leader</li>
</ul></li>
</ul>
<p><strong>Deloitte</strong></p>
<ul>
<li>Juan Tello
<ul>
<li>U.S. CDO, Strategy &amp; Analytics Principal</li>
</ul></li>
<li>Leo Cabrera, Rohit Iyer, and Ajay Tripathi
<ul>
<li>Office of CDO</li>
</ul></li>
</ul>
<p><strong>GM</strong></p>
<ul>
<li>Jon Francis
<ul>
<li>Chief Data &amp; Analytics Officer</li>
</ul></li>
<li>Brian Ames
<ul>
<li>Principal Software Engineer</li>
</ul></li>
</ul>
<p><strong>Howso</strong></p>
<ul>
<li>Chris Hazard
<ul>
<li>Chief Technology Officer</li>
</ul></li>
<li>Michael Meehan
<ul>
<li>General Counsel</li>
</ul></li>
</ul>
<p><strong>Humana</strong></p>
<ul>
<li>Genevy Dimitrion
<ul>
<li>VP, Data Strategy &amp; Governance</li>
</ul></li>
</ul>
<p><strong>IBM</strong></p>
<ul>
<li>Lee Cox
<ul>
<li>VP, Integrated Governance &amp; Market Readiness, Office of Privacy
and Responsible Tech.</li>
</ul></li>
<li>Bryan Bortnick
<ul>
<li>Counsel, Data Governance</li>
</ul></li>
<li>Bryan Kyle
<ul>
<li>Sr. Technical Staff Member, Data Eng., Chief Data Office</li>
</ul></li>
<li>Orla Flannery
<ul>
<li>Privacy Program Manager</li>
</ul></li>
</ul>
<p><strong>Kenvue</strong></p>
<ul>
<li>Bernardo Tavares
<ul>
<li>Chief Technology &amp; Data Officer</li>
</ul></li>
<li>Ajay Dhaul
<ul>
<li>SVP Global Data</li>
</ul></li>
<li>Kim Viccaro
<ul>
<li>Data Strategy &amp; Activation Lead</li>
</ul></li>
<li>Sudheesh Kamath
<ul>
<li>Data &amp; AI Products Leader</li>
</ul></li>
</ul>
<p><strong>Mastercard</strong></p>
<ul>
<li>Travis Carpenter
<ul>
<li>VP, Data Quality &amp; Sources</li>
</ul></li>
<li>Ed Dephilippis
<ul>
<li>VP, Data Management &amp; Quality</li>
</ul></li>
<li>Usha Ramalingam
<ul>
<li>Director, Data Management</li>
</ul></li>
</ul>
<p><strong>Nielsen</strong></p>
<ul>
<li>Christine Pierce
<ul>
<li>Chief Data Officer</li>
</ul></li>
<li>Frank Fasinski
<ul>
<li>Director, Data Science</li>
</ul></li>
</ul>
<p><strong>Nike</strong></p>
<ul>
<li>Emily White
<ul>
<li>Vice President, Enterprise Data &amp; Analytics</li>
</ul></li>
</ul>
<p><strong>Pfizer</strong></p>
<ul>
<li>Peter Hunter
<ul>
<li>Sr Director, Analytic Tools</li>
</ul></li>
<li>Gentiana Spahiu
<ul>
<li>Director, Data Gov. Lead</li>
</ul></li>
<li>Michael Pagliorola, Sasi Mullangi, Adam Nieto, John Pastor, Drew
Palsgrove, and Jay Shetty</li>
</ul>
<p><strong>Regions</strong></p>
<ul>
<li>Dilip Balachandran
<ul>
<li>SVP, Enterprise Data Management</li>
</ul></li>
</ul>
<p><strong>Smithsonian</strong></p>
<ul>
<li>Alan Hejnal
<ul>
<li>Data Quality Manager</li>
</ul></li>
<li>Derrick Whitney
<ul>
<li>Interim CDO</li>
</ul></li>
<li>Adam Soroka
<ul>
<li>Office of Research Computing</li>
</ul></li>
</ul>
<p><strong>Transcarent</strong></p>
<ul>
<li>Thi Montalvo
<ul>
<li>VP Reporting and Analytics</li>
</ul></li>
<li>Thomas Birchfield
<ul>
<li>Technical Program Manager</li>
</ul></li>
</ul>
<p><strong>UPS</strong></p>
<ul>
<li>Mallory Freeman
<ul>
<li>VP, Enterprise Data and Analytics</li>
</ul></li>
<li>Zeenat Syed
<ul>
<li>Director of Data Strategy</li>
</ul></li>
<li>Ricardo Rodriguez
<ul>
<li>Sr Manager, Data Science and Machine Learning</li>
</ul></li>
</ul>
<p><strong>Walmart</strong></p>
<ul>
<li>Gregory Schaffer
<ul>
<li>Chief Counsel, Cyber Security &amp; VP, Digital Trust
Compliance</li>
</ul></li>
<li>Laura Asbury
<ul>
<li>Director, Digital Trust Compliance</li>
</ul></li>
</ul>
<p><strong>Warby Parker</strong></p>
<ul>
<li>Peter Cross
<ul>
<li>Head of Data</li>
</ul></li>
<li>Chris Bleakley
<ul>
<li>Engineering Director</li>
</ul></li>
</ul>
</body>
</html>
